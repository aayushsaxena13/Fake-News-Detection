{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Classifier\n",
    " \n",
    "Data : https://www.kaggle.com/jruvika/fake-news-detection/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide deprecated warnings of sklearn package\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bbc.com/news/world-us-canada-414191...</td>\n",
       "      <td>Four ways Bob Corker skewered Donald Trump</td>\n",
       "      <td>Image copyright Getty Images\\nOn Sunday mornin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.reuters.com/article/us-filmfestiva...</td>\n",
       "      <td>Linklater's war veteran comedy speaks to moder...</td>\n",
       "      <td>LONDON (Reuters) - “Last Flag Flying”, a comed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nytimes.com/2017/10/09/us/politics...</td>\n",
       "      <td>Trump’s Fight With Corker Jeopardizes His Legi...</td>\n",
       "      <td>The feud broke into public view last week when...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.reuters.com/article/us-mexico-oil-...</td>\n",
       "      <td>Egypt's Cheiron wins tie-up with Pemex for Mex...</td>\n",
       "      <td>MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.cnn.com/videos/cnnmoney/2017/10/08/...</td>\n",
       "      <td>Jason Aldean opens 'SNL' with Vegas tribute</td>\n",
       "      <td>Country singer Jason Aldean, who was performin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                URLs  \\\n",
       "0  http://www.bbc.com/news/world-us-canada-414191...   \n",
       "1  https://www.reuters.com/article/us-filmfestiva...   \n",
       "2  https://www.nytimes.com/2017/10/09/us/politics...   \n",
       "3  https://www.reuters.com/article/us-mexico-oil-...   \n",
       "4  http://www.cnn.com/videos/cnnmoney/2017/10/08/...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0         Four ways Bob Corker skewered Donald Trump   \n",
       "1  Linklater's war veteran comedy speaks to moder...   \n",
       "2  Trump’s Fight With Corker Jeopardizes His Legi...   \n",
       "3  Egypt's Cheiron wins tie-up with Pemex for Mex...   \n",
       "4        Jason Aldean opens 'SNL' with Vegas tribute   \n",
       "\n",
       "                                                Body  Label  \n",
       "0  Image copyright Getty Images\\nOn Sunday mornin...      1  \n",
       "1  LONDON (Reuters) - “Last Flag Flying”, a comed...      1  \n",
       "2  The feud broke into public view last week when...      1  \n",
       "3  MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...      1  \n",
       "4  Country singer Jason Aldean, who was performin...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Data/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Fake Articles ->  (2120, 4)\n",
      "Number of Real Articles ->  (1868, 4)\n",
      "Maximum Length  32767\n",
      "Avg Length 2941.288365095286\n"
     ]
    }
   ],
   "source": [
    "# Drop the data with null or undefined values\n",
    "df = df.dropna()\n",
    "# Get the number of each label in the data\n",
    "fake = df[df.Label ==  0]\n",
    "real = df[df.Label ==  1]\n",
    "\n",
    "print('Number of Fake Articles -> ', fake.shape)\n",
    "print('Number of Real Articles -> ', real.shape)\n",
    "\n",
    "# Max Count of words in Document\n",
    "max = 0\n",
    "total = 0\n",
    "count = 0;\n",
    "for i in range(df.shape[0]):\n",
    "    length = len(df.iloc[i,2])\n",
    "    total += length\n",
    "    count += 1\n",
    "    if(length > max):\n",
    "        max = length\n",
    "    \n",
    "print(\"Maximum Length \", max)\n",
    "print(\"Avg Length\", total / count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "Using Stratified sampling, split the data into 70-30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Fake articles in Training set ->  1696\n",
      "Number of Real articles in Training set ->  1494\n",
      "Number of Fake articles in Testing set ->  424\n",
      "Number of Real articles in Testing set ->  374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df.pop('Label')\n",
    "x = df\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "train_count = y_train.value_counts()\n",
    "test_count = y_test.value_counts()\n",
    "\n",
    "print('Number of Fake articles in Training set -> ', train_count[0])\n",
    "print('Number of Real articles in Training set -> ', train_count[1])\n",
    "print('Number of Fake articles in Testing set -> ', test_count[0])\n",
    "print('Number of Real articles in Testing set -> ', test_count[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "* Tokenization\n",
    "* Normalization\n",
    "    * Lowercase all the words\n",
    "    * Negation Handling\n",
    "    * Remove Stopwords\n",
    "    * Remove punctuations and Empty Strings from the array\n",
    "* Stemming\n",
    "\n",
    "Source - https://medium.com/@annabiancajones/sentiment-analysis-of-reviews-text-pre-processing-6359343784fb\n",
    "\n",
    "#### Setup:\n",
    "\n",
    "* Import NLTK\n",
    "* Download and import stopwords from NLTK.corpus\n",
    "* Import PorterStemmer which is the module used for stemming\n",
    "* Import NLTK Vader Sentiment Analysis Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NLTK library and its needed modules\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Import the Vader Sentiment Analysis Library\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the Apostrophes connecting words\n",
    "appos_file = open('appos.txt','r')\n",
    "appos = eval(appos_file.read())\n",
    "appos_file.close()\n",
    "\n",
    "# Function returns the negation handled word if it is presend in the appos dictionary\n",
    "# Else returns the word itself\n",
    "def negationHandling(word):\n",
    "    if word in appos:\n",
    "        return appos[word]\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "# Check if a word is a Stopword\n",
    "# Stopword is a word that is commonly present in most of the documents and does not affect the model\n",
    "def isNotStopWord(word):\n",
    "    return word not in stopwords.words('english')\n",
    "\n",
    "# Function to preprocess a single article\n",
    "# Document refers to the text of the Article.\n",
    "def processDocument(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        #Converting to LowerCase\n",
    "        words = map(str.lower, words)\n",
    "        \n",
    "        # Negation Handling map is'nt to is not : \n",
    "        words = map(lambda x: negationHandling(x), words)\n",
    "        \n",
    "        # Remove stop words\n",
    "        words = filter(lambda x: isNotStopWord(x), words)\n",
    "        \n",
    "        # Removing punctuations except '<.>/<?>/<!>'\n",
    "        punctuations = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'\n",
    "        words = map(lambda x: x.translate(str.maketrans('', '', punctuations)), words)\n",
    "        \n",
    "        # Remove empty strings\n",
    "        words = filter(lambda x: len(x) > 0, words)\n",
    "        \n",
    "        # stemming\n",
    "        words = map(lambda x: ps.stem(x), words)\n",
    "        \n",
    "        # Adding the preprocessed words to the document\n",
    "        tokens = tokens + list(words)\n",
    "        \n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Completed for Body of training data\n",
      "Preprocessing Completed for HeadLine of training data\n"
     ]
    }
   ],
   "source": [
    "# Processing the body i.e. text of the Article\n",
    "train_body = x_train.loc[:,'Body']\n",
    "train_raw_body = []\n",
    "train_body_sentiment = []\n",
    "\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_body_sentiment.append(list(analyser.polarity_scores(train_body.iloc[i]).values()))\n",
    "    train_raw_body.append(train_body.iloc[i])\n",
    "        \n",
    "train_body_wordArray = list(map(lambda x: processDocument(x), train_raw_body))\n",
    "print(\"Preprocessing Completed for Body of training data\")\n",
    "\n",
    "# Process the Headlines of the training data.\n",
    "train_headline = x_train.loc[:,'Headline']\n",
    "train_raw_headline = []\n",
    "train_headline_sentiment = []\n",
    "\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_headline_sentiment.append(list(analyser.polarity_scores(train_headline.iloc[i]).values()))\n",
    "    train_raw_headline.append(train_headline.iloc[i])\n",
    "        \n",
    "train_headline_wordArray = list(map(lambda x: processDocument(x), train_raw_headline))\n",
    "print(\"Preprocessing Completed for HeadLine of training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec Model\n",
      "Training iteration 10\n",
      "Training iteration 20\n",
      "Training iteration 30\n",
      "Training iteration 40\n",
      "Training iteration 50\n",
      "Training iteration 60\n",
      "Training iteration 70\n",
      "Training iteration 80\n",
      "Training iteration 90\n",
      "Training iteration 100\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "tagged_body_data = [TaggedDocument(\n",
    "    words = train_body_wordArray[i], \n",
    "    tags = [str(i)]) for i, _d in enumerate(train_body_wordArray)]\n",
    "\n",
    "max_epochs = 100\n",
    "vec_size = 300\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(\n",
    "    vector_size = vec_size,\n",
    "    alpha = alpha, \n",
    "    min_alpha = 0.025,\n",
    "    min_count = 5,\n",
    "    window = 10,\n",
    "    dm = 1)\n",
    "\n",
    "model.build_vocab(tagged_body_data)\n",
    "print('Training Doc2Vec Model')\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    if ((epoch + 1) % 10 == 0):\n",
    "        print('Training iteration {0}'.format(epoch + 1))\n",
    "    model.train(tagged_body_data,\n",
    "                total_examples = model.corpus_count,\n",
    "                epochs = model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Completed for Body of testing data\n",
      "Preprocessing Completed for HeadLine of testing data\n"
     ]
    }
   ],
   "source": [
    "# Pre-process the body of the article of test Data set \n",
    "test_body = x_test.loc[:,'Body']\n",
    "test_raw_body = []\n",
    "test_body_sentiment = []\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_body_sentiment.append(list(analyser.polarity_scores(test_body.iloc[i]).values()))\n",
    "    test_raw_body.append(test_body.iloc[i])\n",
    "        \n",
    "test_body_wordArray = list(map(lambda x: processDocument(x), test_raw_body))\n",
    "print(\"Preprocessing Completed for Body of testing data\")\n",
    "\n",
    "# Preprocess the Headline of the article for testing dataset\n",
    "test_headLine = x_test.loc[:,'Headline']\n",
    "test_raw_headline = []\n",
    "test_headline_sentiment = []\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_headline_sentiment.append(list(analyser.polarity_scores(test_body.iloc[i]).values()))\n",
    "    test_raw_headline.append(test_headLine.iloc[i])\n",
    "        \n",
    "test_headline_wordArray = list(map(lambda x: processDocument(x), test_raw_headline))\n",
    "print(\"Preprocessing Completed for HeadLine of testing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Contains the list of Fake News website URL's\n",
    "fake_URL_df = pd.read_csv(\"Data/fake_news_websites.csv\")\n",
    "# Contains the list of Authentic News website URL's\n",
    "fact_URL_df = pd.read_csv(\"Data/fact_news_websites.csv\")\n",
    "\n",
    "##Computes the similarity score between 2 strings\n",
    "def similarityRatio(url_one, url_two):\n",
    "    return SequenceMatcher(None, url_one, url_two).ratio()\n",
    "\n",
    "##Extracts the domain from a URL - for example: 'https://www.bbc.com' will become 'bbc'\n",
    "def getDomain(url):\n",
    "    return url.lstrip(\"https://www.\").split(\".\")[0]\n",
    "\n",
    "##Assigns a score to the URL by string matching with URL of fake websites\n",
    "def fakeURLCheckAssign(url):\n",
    "    minSimilarity = 0.5\n",
    "    for i in range(len(fake_URL_df['SiteName'])):\n",
    "        similarity = similarityRatio(getDomain(url),getDomain(fake_URL_df['SiteName'][i]))\n",
    "        if (similarity > 0.75):\n",
    "            minSimilarity = min(minSimilarity,(1 - similarity))\n",
    "            \n",
    "    return minSimilarity\n",
    "\n",
    "##Assigns a score to the URL by string matching with URL of authentic websites\n",
    "def factURLCheckAssign(url):\n",
    "    for i in range(len(fact_URL_df['SiteName'])):\n",
    "        minSimilarity = 0.5\n",
    "        similarity = similarityRatio(getDomain(url), getDomain(fact_URL_df['SiteName'][i]))\n",
    "\n",
    "        if similarity == 1:\n",
    "            return similarity\n",
    "            \n",
    "        if similarity > 0.75:\n",
    "            minSimilarity = min(minSimilarity,1 - similarity)\n",
    "            \n",
    "    return minSimilarity\n",
    "          \n",
    "def URLScore(url):\n",
    "    fakeScore = fakeURLCheckAssign(url)\n",
    "    factScore = factURLCheckAssign(url)\n",
    "    if factScore == 1:\n",
    "        return 1\n",
    "    if fakeScore == 0:\n",
    "        return 0\n",
    "    if factScore == 0.5 and fakeScore == 0.5:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return min(fakeScore, factScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_URL = x_test.loc[:,'URLs']\n",
    "train_URL = x_train.loc[:,'URLs']\n",
    "# Higher Score denotes that the article is more authentic\n",
    "# Completely Real Article URL Score - 1 & Completely Fake Article URL Score - 0\n",
    "\n",
    "# Training data set URL Score\n",
    "train_URLScore_vector = []    \n",
    "for i in range(x_train.shape[0]):\n",
    "    train_URLScore_vector.append(URLScore(train_URL.iloc[i]))\n",
    "\n",
    "# Testing data set URL Score\n",
    "test_URLScore_vector = []    \n",
    "for i in range(x_test.shape[0]):\n",
    "    test_URLScore_vector.append(URLScore(test_URL.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.125, 1, 1, 0.23076923076923073, 1, 0.5, 1, 1, 0.125, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.125, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.125, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.5, 0.125, 0.5, 0.5, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 0.125, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 1, 0.5, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.5, 1, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.125, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.5, 1, 1, 1, 1, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.5, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.5, 1, 0.23076923076923073, 0.5, 1, 1, 1, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 1, 1, 0.23076923076923073, 0.5, 0.5, 0.5, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.125, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.125, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.125, 0.5, 1, 0.5, 1, 1, 0.23076923076923073, 1, 1, 0.5, 1, 1, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.125, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.125, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 1, 1, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 1, 1, 1, 0.23076923076923073, 1, 0.5, 1, 1, 0.5, 0.5, 1, 1, 0.5, 1, 0.5, 0.23076923076923073, 0.5, 0.5, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.5, 0.125, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 1, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.5, 1, 0.5, 1, 1, 0.23076923076923073, 1, 0.125, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.125, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.5, 1, 1, 0.23076923076923073, 1, 1, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.125, 0.5, 1, 1, 1, 0.23076923076923073, 1, 0.5, 0.5, 1, 1, 0.5, 1, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.125, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 1, 1, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.125, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 1, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.125, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.125, 1, 0.5, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.5, 1, 0.23076923076923073, 1, 1, 1, 0.5, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 1, 1, 0.5, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.125, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.5, 0.125, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.5, 0.5, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.5, 1, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 0.5, 1, 0.125, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.125, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.5, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.125, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.125, 0.5, 1, 0.5, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.125, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.5, 0.23076923076923073, 0.5, 0.5, 1, 0.5, 0.5, 1, 1, 1, 0.5, 1, 0.5, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.5, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.125, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.125, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.5, 0.125, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.5, 1, 0.5, 1, 0.5, 1, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 0.5, 0.5, 0.5, 1, 1, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.5, 0.5, 0.125, 0.5, 0.5, 0.23076923076923073, 0.5, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 1, 1, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.125, 1, 0.5, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 1, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.125, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 1, 0.5, 0.5, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 1, 0.5, 0.5, 1, 0.23076923076923073, 1, 0.5, 0.125, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.125, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 0.5, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.5, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.125, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 1, 1, 0.23076923076923073, 1, 1, 0.5, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.125, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.5, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.125, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.5, 1, 0.5, 0.5, 0.5, 0.23076923076923073, 1, 1, 1, 0.5, 1, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 1, 0.5, 0.5, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.125, 0.5, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 1, 1, 0.125, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.125, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.125, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 1, 0.5, 0.5, 0.5, 1, 0.5, 1, 1, 1, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 1, 1, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.5, 0.5, 1, 1, 0.23076923076923073, 1, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.125, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.125, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 1, 1, 0.5, 0.23076923076923073, 1, 0.125, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 1, 0.5, 0.23076923076923073, 1, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.125, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.125, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 1, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.5, 1, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 1, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.125, 0.5, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.5, 1, 1, 0.5, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073]\n",
      "[1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.125, 0.23076923076923073, 0.5, 1, 1, 1, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.5, 0.5, 1, 0.5, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.125, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 0.5, 0.23076923076923073, 0.5, 0.5, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.5, 1, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 1, 0.5, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.5, 1, 1, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.125, 0.5, 0.5, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.5, 1, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 1, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.125, 1, 1, 1, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.5, 1, 1, 0.5, 0.5, 0.5, 1, 1, 0.5, 1, 1, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.5, 1, 0.5, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 0.125, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 0.5, 1, 0.125, 0.125, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.5, 0.5, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.5, 1, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.5, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 0.5, 1, 0.23076923076923073, 0.5, 0.5, 1, 0.5, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 1, 1, 0.5, 0.23076923076923073, 0.5, 1, 0.5, 0.5, 1, 1, 0.5, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.5, 1, 1, 0.125, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.23076923076923073, 1, 0.5, 1, 1, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 1, 0.5, 0.5, 1, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 1, 0.5, 1, 0.23076923076923073, 1, 0.125, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.5, 0.5, 0.23076923076923073, 0.5, 1, 0.23076923076923073, 0.5, 1, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.5, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.5, 1, 0.5, 1, 0.23076923076923073, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.5, 0.5, 0.23076923076923073, 0.23076923076923073, 1, 1, 1, 1, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.23076923076923073, 1, 0.23076923076923073, 0.23076923076923073, 1, 0.23076923076923073, 0.5, 0.5, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_URLScore_vector)\n",
    "print(test_URLScore_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word vectors using the trained doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the np training data (3190, 609)\n",
      "Shape of the np training data (798, 609)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model = Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "# Training set Body Word Vector  \n",
    "train_body_vector = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    trainBodyConcat = np.concatenate([model.docvecs[i], np.asarray(train_body_sentiment[i])])\n",
    "    train_body_vector.append(trainBodyConcat)\n",
    "\n",
    "# Training data set Headline Word Vectors\n",
    "train_headline_vector = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_headline_sentiment[i].append(train_URLScore_vector[i])\n",
    "    trainHeadConcat = np.concatenate([model.infer_vector(train_headline_wordArray[i]), \n",
    "                                      np.asarray(train_headline_sentiment[i])]) \n",
    "    train_headline_vector.append(trainHeadConcat)\n",
    "\n",
    "# Testing set Body Word Vector\n",
    "test_body_vector = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    testBodyConcat = np.concatenate([model.infer_vector(test_body_wordArray[i]), \n",
    "                                     np.asarray(test_body_sentiment[i])])\n",
    "    test_body_vector.append(testBodyConcat)\n",
    "    \n",
    "# Testing set Headline Word Vectors\n",
    "test_headline_vector = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_headline_sentiment[i].append(test_URLScore_vector[i])\n",
    "    testHeadConcat = np.concatenate([model.infer_vector(test_headline_wordArray[i]), \n",
    "                                     np.asarray(test_headline_sentiment[i])])\n",
    "    test_headline_vector.append(testHeadConcat)    \n",
    "\n",
    "# Create Numpy Array for training data to train sklearn models\n",
    "np_train_headline = np.array([np.array(xi) for xi in train_headline_vector]) \n",
    "np_train_body = np.array([np.array(xi) for xi in train_body_vector])\n",
    "\n",
    "inp_x_train = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    inp_x_train.append(np.concatenate((np_train_headline[i], np_train_body[i])))\n",
    "\n",
    "inp_x_train = np.array(inp_x_train)\n",
    "\n",
    "# Create np Array for testing data to train sklearn models\n",
    "np_test_headline = np.array([np.array(xi) for xi in test_headline_vector])\n",
    "np_test_body = np.array([np.array(xi) for xi in test_body_vector])\n",
    "\n",
    "inp_x_test = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    inp_x_test.append(np.concatenate((np_test_headline[i], np_test_body[i])))\n",
    "\n",
    "inp_x_test = np.array(inp_x_test)\n",
    "\n",
    "print('Shape of the np training data', inp_x_train.shape)\n",
    "print('Shape of the np training data', inp_x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found : {'C': 0.1, 'kernel': 'linear'}\n",
      "accuracy -> 0.937\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "C = [0.1, 0.5, 1, 5, 10, 50]\n",
    "param_grid = [\n",
    "     {'C': C, 'kernel': ['linear']},\n",
    "     {'C': C, 'gamma': [0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']},\n",
    "     {'degree': [2,3,4], 'kernel': ['poly']},\n",
    "     {'coef0': [0.0], 'kernel': ['sigmoid']} \n",
    "]\n",
    "\n",
    "table = {}\n",
    "\n",
    "score_metric = 'accuracy'\n",
    "clf = GridSearchCV(svm.SVC(), param_grid, cv = 5, scoring = score_metric)\n",
    "clf.fit(inp_x_train, y_train)\n",
    "print(\"Best parameters set found :\", clf.best_params_)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "\n",
    "for mean, params in zip(means, clf.cv_results_['params']):\n",
    "    if params == clf.best_params_:\n",
    "        print(\"%s -> %0.3f\" % (score_metric, mean))\n",
    "        key = str(params)\n",
    "    if key not in table:\n",
    "        table[key] = []\n",
    "        table[key].append(\"%0.3f\" % (mean))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "Using GridSearchCV to find the optimal parameters for this training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the SVC with above parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  0.9210526315789473\n",
      "Precision ->  0.9239792535716824\n",
      "Recall ->  0.9210526315789473\n",
      "F-Score ->  0.9211256938956043\n",
      "Support ->  None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "#svm_model = svm.SVC(C = 10, gamma = 0.0001, kernel = 'rbf')\n",
    "svm_model = svm.SVC(C = 0.1, kernel = 'linear')\n",
    "svm_model.fit(inp_x_train, y_train)\n",
    "y_pred = svm_model.predict(inp_x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy -> ', accuracy)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average = 'weighted')\n",
    "print('Precision -> ', precision)\n",
    "print('Recall -> ', recall)\n",
    "print('F-Score -> ', fscore)\n",
    "print('Support -> ', support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  0.5626566416040101\n",
      "Precision ->  0.7601117930661063\n",
      "Recall ->  0.5626566416040101\n",
      "F-Score ->  0.43514383284862107\n",
      "Support ->  None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(inp_x_train, y_train)\n",
    "\n",
    "nb_y_pred = gnb.predict(inp_x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, nb_y_pred)\n",
    "print('Accuracy -> ', accuracy)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, nb_y_pred, average = 'weighted')\n",
    "print('Precision -> ', precision)\n",
    "print('Recall -> ', recall)\n",
    "print('F-Score -> ', fscore)\n",
    "print('Support -> ', support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  0.931077694235589\n",
      "Precision ->  0.9340160315027508\n",
      "Recall ->  0.931077694235589\n",
      "F-Score ->  0.9311414787977496\n",
      "Support ->  None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(inp_x_train, y_train)\n",
    "\n",
    "dt_y_pred = dt_clf.predict(inp_x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, dt_y_pred)\n",
    "print('Accuracy -> ', accuracy)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, dt_y_pred, average = 'weighted')\n",
    "print('Precision -> ', precision)\n",
    "print('Recall -> ', recall)\n",
    "print('F-Score -> ', fscore)\n",
    "print('Support -> ', support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "#### Neural Network\n",
    "\n",
    "* Import Tensorflow\n",
    "* Import Keras\n",
    "* Set the seed value to 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 1s - loss: 0.2610 - acc: 0.4759\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.2494 - acc: 0.5317\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.2415 - acc: 0.5317\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.2159 - acc: 0.5574\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.1932 - acc: 0.7931\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.1773 - acc: 0.8417\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.1654 - acc: 0.8680\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.1560 - acc: 0.8881\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.1478 - acc: 0.8991\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.1407 - acc: 0.9056\n",
      "798/798 [==============================] - 0s 62us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2613 - acc: 0.6185\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1817 - acc: 0.7455\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.1311 - acc: 0.8276\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0999 - acc: 0.8749\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0815 - acc: 0.9003\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0685 - acc: 0.9197\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0601 - acc: 0.9320\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0536 - acc: 0.9420\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0485 - acc: 0.9476\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0454 - acc: 0.9514\n",
      "798/798 [==============================] - 0s 106us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2600 - acc: 0.5749\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.2295 - acc: 0.6536\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.1923 - acc: 0.7254\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.1394 - acc: 0.8066\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.1054 - acc: 0.8618\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0851 - acc: 0.8903\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0732 - acc: 0.9129\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0631 - acc: 0.9260\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0563 - acc: 0.9348\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0518 - acc: 0.9395\n",
      "798/798 [==============================] - 0s 63us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2240 - acc: 0.6887\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1338 - acc: 0.8335\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0920 - acc: 0.8915\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0703 - acc: 0.9185\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0582 - acc: 0.9345\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0479 - acc: 0.9461\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0411 - acc: 0.9564\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0355 - acc: 0.9652\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0311 - acc: 0.9730\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0275 - acc: 0.9765\n",
      "798/798 [==============================] - 0s 56us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2719 - acc: 0.6163\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1613 - acc: 0.7991\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.1175 - acc: 0.8614\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0939 - acc: 0.8915\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0779 - acc: 0.9150\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0671 - acc: 0.9282\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0585 - acc: 0.9379\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0523 - acc: 0.9455\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0468 - acc: 0.9511\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0431 - acc: 0.9564\n",
      "798/798 [==============================] - 0s 62us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2371 - acc: 0.6790\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1287 - acc: 0.8382\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0849 - acc: 0.8984\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0635 - acc: 0.9307\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0510 - acc: 0.9429\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0428 - acc: 0.9574\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0365 - acc: 0.9618\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0315 - acc: 0.9715\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0274 - acc: 0.9749\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0244 - acc: 0.9793\n",
      "798/798 [==============================] - 0s 64us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2128 - acc: 0.7094\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1143 - acc: 0.8734\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0791 - acc: 0.9107\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0612 - acc: 0.9317\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0494 - acc: 0.9473\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0417 - acc: 0.9567\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0361 - acc: 0.9627\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0315 - acc: 0.9712\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0277 - acc: 0.9759\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0246 - acc: 0.9803\n",
      "798/798 [==============================] - 0s 64us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2146 - acc: 0.6987\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1175 - acc: 0.8533\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0799 - acc: 0.9019\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0602 - acc: 0.9329\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0488 - acc: 0.9445\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0406 - acc: 0.9605\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0346 - acc: 0.9649\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0296 - acc: 0.9712\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0254 - acc: 0.9755\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0221 - acc: 0.9818\n",
      "798/798 [==============================] - 0s 63us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2701 - acc: 0.6188\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1175 - acc: 0.8542\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0793 - acc: 0.9060\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0615 - acc: 0.9310\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0509 - acc: 0.9476\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0440 - acc: 0.9577\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0379 - acc: 0.9643\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0337 - acc: 0.9690\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0301 - acc: 0.9724\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0270 - acc: 0.9768\n",
      "798/798 [==============================] - 0s 58us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2388 - acc: 0.6702\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1102 - acc: 0.8596\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0719 - acc: 0.9103\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0555 - acc: 0.9317\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0451 - acc: 0.9486\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0376 - acc: 0.9602\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0332 - acc: 0.9668\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0286 - acc: 0.9715\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0244 - acc: 0.9768\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0211 - acc: 0.9821\n",
      "798/798 [==============================] - 0s 65us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2314 - acc: 0.6856\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1016 - acc: 0.8746\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0660 - acc: 0.9188\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0501 - acc: 0.9439\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0410 - acc: 0.9561\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0347 - acc: 0.9643\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0307 - acc: 0.9705\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0255 - acc: 0.9755\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0217 - acc: 0.9812\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0199 - acc: 0.9859\n",
      "798/798 [==============================] - 0s 51us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.1996 - acc: 0.7241\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0919 - acc: 0.8831\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0620 - acc: 0.9251\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0470 - acc: 0.9455\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0378 - acc: 0.9592\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0304 - acc: 0.9708\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0256 - acc: 0.9784\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0216 - acc: 0.9828\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0184 - acc: 0.9853\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0155 - acc: 0.9893\n",
      "798/798 [==============================] - 0s 40us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2298 - acc: 0.6918\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1017 - acc: 0.8730\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0671 - acc: 0.9223\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0515 - acc: 0.9433\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0423 - acc: 0.9561\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0348 - acc: 0.9652\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0289 - acc: 0.9730\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0236 - acc: 0.9806\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0205 - acc: 0.9840\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0177 - acc: 0.9859\n",
      "798/798 [==============================] - 0s 57us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.1944 - acc: 0.7354\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0884 - acc: 0.8878\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0602 - acc: 0.9282\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0450 - acc: 0.9489\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0366 - acc: 0.9614\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0307 - acc: 0.9721\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0260 - acc: 0.9749\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0221 - acc: 0.9793\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0188 - acc: 0.9856\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0162 - acc: 0.9884\n",
      "798/798 [==============================] - 0s 62us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2127 - acc: 0.7116\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0866 - acc: 0.8871\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0554 - acc: 0.9301\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0396 - acc: 0.9574\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0310 - acc: 0.9705\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0251 - acc: 0.9806\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0214 - acc: 0.9831\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0174 - acc: 0.9887\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0149 - acc: 0.9900\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0130 - acc: 0.9915\n",
      "798/798 [==============================] - 0s 60us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2458 - acc: 0.6571\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1065 - acc: 0.8671\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0682 - acc: 0.9204\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0528 - acc: 0.9398\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0425 - acc: 0.9536\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0361 - acc: 0.9614\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0309 - acc: 0.9696\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0252 - acc: 0.9755\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0216 - acc: 0.9799\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0187 - acc: 0.9859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798/798 [==============================] - 0s 45us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2121 - acc: 0.7188\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0860 - acc: 0.8928\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0579 - acc: 0.9317\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0436 - acc: 0.9511\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0352 - acc: 0.9630\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0287 - acc: 0.9740\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0234 - acc: 0.9799\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0201 - acc: 0.9840\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0176 - acc: 0.9881\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0152 - acc: 0.9890\n",
      "798/798 [==============================] - 0s 39us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2259 - acc: 0.6871\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0936 - acc: 0.8881\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0628 - acc: 0.9260\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0471 - acc: 0.9483\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0358 - acc: 0.9624\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0282 - acc: 0.9730\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0235 - acc: 0.9812\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0189 - acc: 0.9850\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0156 - acc: 0.9893\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0135 - acc: 0.9909\n",
      "798/798 [==============================] - 0s 60us/step\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2102 - acc: 0.7125\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0829 - acc: 0.8928\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0569 - acc: 0.9345\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0440 - acc: 0.9536\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0363 - acc: 0.9624\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0301 - acc: 0.9721\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0249 - acc: 0.9771\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0213 - acc: 0.9837\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0182 - acc: 0.9865\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0154 - acc: 0.9897\n",
      "798/798 [==============================] - 0s 60us/step\n",
      "{2: 0.9056426331541008, 4: 0.9514106624178752, 6: 0.9394984288275429, 8: 0.9764890375555869, 10: 0.9564263417055614, 12: 0.9793103535347224, 14: 0.9802507907619297, 16: 0.9818181905626877, 18: 0.9768025157593635, 20: 0.982131673624523, 22: 0.9858934292598952, 24: 0.989341701833432, 26: 0.9858934268308658, 28: 0.9884012649799215, 30: 0.9915360571075009, 32: 0.9858934285125015, 34: 0.9890282217611713, 36: 0.990909098457767, 38: 0.9896551792898148}\n",
      "{2: 0.7869674254180794, 4: 0.8245613999235302, 6: 0.8208019888789433, 8: 0.852130321332984, 10: 0.8032581546252832, 12: 0.8358395951134818, 14: 0.8421052601701933, 16: 0.8308270615443849, 18: 0.8395989870367792, 20: 0.8295739270690689, 22: 0.8333333173491303, 24: 0.7832080128796417, 26: 0.8270676575208965, 28: 0.8182957429336127, 30: 0.7769423546946437, 32: 0.8208019993358985, 34: 0.8233082769508648, 36: 0.8195488614247257, 38: 0.8270676643926099}\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "# Set the seed as 7 to get reproducible results\n",
    "seed(7)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set the seed as 7 here as well\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(7)\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "\n",
    "train_accuracy = {}\n",
    "test_accuracy = {}\n",
    "models = []\n",
    "\n",
    "for i in range(1,20):\n",
    "    no_of_hidden_neurons = i * 2\n",
    "    \n",
    "    # Create a new Model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(no_of_hidden_neurons, input_dim = inp_x_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(loss = 'mse',optimizer = 'adam',metrics = ['accuracy'])\n",
    "    \n",
    "    history = model.fit(inp_x_train, y_train, epochs = 10, batch_size = 100, verbose=2)\n",
    "    train_accuracy[no_of_hidden_neurons] = history.history['acc'][9]\n",
    "    \n",
    "    # Store the model\n",
    "    models.append(model)\n",
    "    \n",
    "    # Calculate the score on the testing data set\n",
    "    test_scores = model.evaluate(inp_x_test, y_test, batch_size = 100)\n",
    "    test_accuracy[no_of_hidden_neurons] = test_scores[1]\n",
    "    # Reset keras and tf\n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "print(train_accuracy)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
